# -*- coding: utf-8 -*-
"""data_practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qSXhkrPzs3XmC7gwwLWJE6B17-fu_g_X

Review:

Table is called a dataframe short for df and a columm is called a series

useful df functions:

.head, .shape, .info, .describe

to access specific column will use df_name["column name"] and then filter rows as  dog[dogs["date of birth"] > 50]

for categricla data use the isin method to filter rows.

sorting example:

# Sort homelessness by descending family members
homelessness_fam = homelessness.sort_values("family_members", ascending=False)

# Sort homelessness by region, then descending family members
homelessness_reg_fam = homelessness.sort_values(["region", "family_members"], ascending=[True, False])

# Print the top few rows
print(homelessness_reg_fam.head())

# Filter categorical data
canu = ["California", "Arizona", "Nevada", "Utah"]

mojave_homelessness = homelessness[homelessness["state"].isin(canu)]

print(mojave_homelessness)

# Using filter and creating new columsn to find specific data to find "Which state has the highest number of homeless individuals per 10,000 people in the state?" Combine your new pandas skills to find out

-> Create indiv_per_10k col as homeless individuals per 10k state pop

homelessness["indiv_per_10k"] = 10000 * homelessness["individuals"] / homelessness["state_pop"]

-> Subset rows for indiv_per_10k greater than 20

high_homelessness = homelessness[homelessness["indiv_per_10k"] > 20]

-> Sort high_homelessness by descending indiv_per_10k

high_homelessness_srt = high_homelessness.sort_values("indiv_per_10k", ascending=False)

-> From high_homelessness_srt, select the state and indiv_per_10k cols

result = high_homelessness_srt[["state","indiv_per_10k"]]

-> See the result
print(result)

Project:
"""

import numpy as np #  particularly useful when dealing with arrays and matrices
import pandas as pd #Pandas is a powerful data manipulation and analysis library built on top of NumPy.
# It's particularly well-suited for handling structured data and is widely used in data science and machine learning workflows.
import matplotlib.pyplot as plt

"""- Use scikit-learn for traditional machine learning tasks, rapid prototyping, small to medium-sized datasets, and standard preprocessing tasks.
- Use TensorFlow for deep learning, handling large-scale datasets, production deployment, advanced machine learning techniques, and tasks requiring high flexibility and performance.
"""

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""Visual Example
Classification Example:

Task: Classify whether a customer will buy a product (Yes/No).
Input Features: Age, Income, Browsing History.
Output: "Yes" or "No".
Regression Example:

Task: Predict the price of a house.
Input Features: Size of the house, Number of bedrooms, Location.
Output: A continuous value representing the price of the house (e.g., $250,000).
"""

# reading in our data using .read_csv another ways are
# - pd.read_excel, pd.read_sql, pd.read_json, pd.read_html,

df = pd.read_csv("/content/Sleep_health_and_lifestyle_dataset.csv")

"""# As we read this csv file what can we learn from this data"""

